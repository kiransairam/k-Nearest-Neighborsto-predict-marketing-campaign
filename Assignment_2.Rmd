---
title: "Assignment_2"
output:
  html_document:
    df_print: paged
date: "2023-02-21"
---
Data Exploration

Open the file bank-names.txt and carefully read the attribute information to understand what information is stored in each attribute, what values each attribute can take and so on.

1. (1pt) Download the dataset and store it in a dataframe in R. Note: the attributes are separated by semicolon
so make sure you set “sep” option correctly inside read.csv
```{R}

bank_dataset = read.csv("C:/ML/bank-full.csv", sep=";", na.strings = c("unknown",""))

```

2. (2 pt) Explore the overall structure of the dataset using the str() function. Get a summary statistics of each variable. Explain what is the type of each variable ( categorical( unordered), categorical (ordered), or continuous).

```{R}
str(bank_dataset)
summary(bank_dataset)

```

age: continuous
job: categorical (unordered)
marital: categorical (unordered)
education: categorical (unordered)
default: categorical (unordered)
balance: continuous
housing: categorical (unordered)
loan: categorical (unordered)
contact: categorical (unordered)
day: continuous
month: categorical (ordered)
duration: continuous
campaign: continuous
pdays: continuous
previous: continuous
poutcome: categorical (unordered)
y: categorical (unordered)



3. (1pt) Get the frequency table of the target variable “y” to see how many observations you have in each category of y. Is y balanced? that is, do you have roughly same observations in y=yes and y=no?

```{R}

table(bank_dataset$y)

```

```{R}

y_freq = prop.table(table(bank_dataset$y)) * 100
y_freq

```
This output shows that 88.3% of the observations fall into the y category of "no," whereas 11.7% go into the y category of "yes." Because there are noticeably more observations in the "no" category than in the "yes" category, we can infer that y is not balanced.



4. (3 pts) Explore the data in order to investigate the association between the target variable y and other variables in the dataset. Which of the other variables are associated with y? Use appropriate plots and statistic tests to answer this question.



```{R}

mosaicplot(y~job, data = bank_dataset)
mosaicplot(y~marital, data = bank_dataset)
mosaicplot(y~education, data = bank_dataset)
mosaicplot(y~default, data = bank_dataset)
mosaicplot(y~housing, data = bank_dataset)
mosaicplot(y~loan, data = bank_dataset)
mosaicplot(y~contact, data = bank_dataset)
mosaicplot(y~month, data = bank_dataset)
mosaicplot(y~poutcome, data = bank_dataset)

```

```{R}

test1 = chisq.test(table(bank_dataset$job , bank_dataset$y ))
test2 = chisq.test(table(bank_dataset$marital , bank_dataset$y ))
test3 = chisq.test(table(bank_dataset$education , bank_dataset$y ))
test4 = chisq.test(table(bank_dataset$default , bank_dataset$y ))
test5 = chisq.test(table(bank_dataset$housing , bank_dataset$y ))
test6 = chisq.test(table(bank_dataset$loan , bank_dataset$y))
test7 = chisq.test(table(bank_dataset$contact , bank_dataset$y))
test8 = chisq.test(table(bank_dataset$month , bank_dataset$y))
test9 = chisq.test(table(bank_dataset$poutcome , bank_dataset$y ))

test1
test2
test3
test4
test5
test6
test7
test8
test9



```

Based on the Pearson's chi-squared tests that you have provided, all variables seem to be significantly associated with the outcome variable, y. This includes the job, marital status, education, default, housing, loan, contact, month, and poutcome variables. The null hypothesis for these tests is that there is no association between the two variables, while the alternative hypothesis is that there is an association. Since all p-values are less than 0.05, we reject the null hypothesis and conclude that there is an association between each variable and the outcome variable.


```{R}
sapply(bank_dataset[, c("age", "y")], function(x) any(!is.numeric(x)))

```
```{R}

bank_dataset$y_binary = ifelse(bank_dataset$y == "yes", 1, 0)

```

```{R}

str(bank_dataset)

```


```{R}

#install.packages("ggplot2")

library(ggplot2)

ggplot(bank_dataset, aes(x = y, y = age, fill = y)) + 
  geom_violin() + 
  labs(title = "Distribution of age by y") + 
  xlab("y") + 
  ylab("age")


boxplot(bank_dataset$balance ~ bank_dataset$y_binary,
        col='red',
        main='association between the y and balance',
        xlab='balance',
        ylab='y')

boxplot(bank_dataset$day ~ bank_dataset$y_binary,
        col='red',
        main='association between the y and day',
        xlab='day',
        ylab='y')

boxplot(bank_dataset$duration ~ bank_dataset$y_binary,
        col='red',
        main='association between the y and duration',
        xlab='duration',
        ylab='y')

boxplot(bank_dataset$campaign ~ bank_dataset$y_binary,
        col='red',
        main='association between the y and campaign',
        xlab='campaign',
        ylab='y')

boxplot(bank_dataset$pdays ~ bank_dataset$y_binary,
        col='red',
        main='association between the y and pdays',
        xlab='pdays',
        ylab='y')

boxplot(bank_dataset$previous ~ bank_dataset$y_binary,
        col='red',
        main='association between the y and previous',
        xlab='previous',
        ylab='y')




```

```{R}

t.test(bank_dataset$age~bank_dataset$y_binary)
t.test(bank_dataset$balance~bank_dataset$y_binary)
t.test(bank_dataset$day~bank_dataset$y_binary)
t.test(bank_dataset$duration~bank_dataset$y_binary)
t.test(bank_dataset$campaign~bank_dataset$y_binary)
t.test(bank_dataset$pdays~bank_dataset$y_binary)
t.test(bank_dataset$previous~bank_dataset$y_binary)

```


Based on the statistical analysis we performed, it appears that the following variables have a significant association with the target variable y and y_binary.

age
balance
campaign
pdays
previous
job
marital
education
default
housing
loan
contact
month
poutcome

We should keep these variables and remove the others from the dataset. those are day and duration


```{R}

bank_dataset = bank_dataset[, !(names(bank_dataset) %in% c("day", "duration","y_binary"))]

```

```{R}

str(bank_dataset)

```



Data Preparation:



5. (1pt) Use the command colSums(is.na(<your dataframe>) to get the number of missing valuesin each column of your dataframe. Which columns have missing values? Note: some variables use “unknown” for missing values. Convert all “unknown” values to NA. You can do so by setting “na.strings” parameter to “unknown” when you read the file using read.csv.


```{R}

colSums(is.na(bank_dataset))

```
Based on the above states, we can assume that there missing values in the Job,Education,Contact,poutcome.



6. (3 pt) There are several ways we can deal with missing values. The easiest approach isto remove all the rows with missing values. However, if a large number of rows have missing values removing them will result in loss of information and may affect the classifier performance. If a large number
of rows have missing values, then it is typically better to substitute missing values. This is called data imputation. Several methods for missing data imputation exist. The most naïve method (which we will use here) is to replace the missing values with mean of the column (for a numerical column) or mode/majority value of the column (for a categorical column). We will use a more advanced data imputation method in a later module. For now, replace the missing values in a numerical column with the mean of the column and the missing values in a categorical column with the mode/majority of the column. After imputation, use colSums(is.na(<your dataframe>) to make sure that your dataframe no longer has missing values.

```{R}

bank_dataset$job[is.na(bank_dataset$job)]=names(sort(-table(bank_dataset$job)))[1]


bank_dataset$education[is.na(bank_dataset$education)]=names(sort(-table(bank_dataset$education)))[1]


bank_dataset$contact[is.na(bank_dataset$contact)]=names(sort(-table(bank_dataset$contact)))[1]

# Replace missing values in poutcome column with "unknown"
bank_dataset$poutcome[is.na(bank_dataset$poutcome)] = "unknown"


```

```{R}

colSums(is.na(bank_dataset))

```

7. Set the seed of the random number generator to a fixed integer, say 1, so that I can reproduce your work:

```{R}

set.seed(1)

```


8. (1pt) Randomize the order of the rowsin the dataset

```{R}

bank_dataset = bank_dataset[sample(nrow(bank_dataset)),]

```

9. (2 pt) This dataset has several categorical variables. With the exception of few models ( such as Naiive Bayes and tree-based models) most machine learning models require numeric features and cannot work directly with categorical data. One way to deal with categorical variables is to assign numeric indices to each level. However, this imposes an artificial ordering on an unordered categorical variable. For example, suppose that we have a categorical variable primary color with three levels: “red”,”blue”,”green”. If we convert “red” to 0 , “blue” to 1 and “green” to 2 then we are telling our model that red < blue< green which is not correct. A better way to encode an unordered categorical variable is to do one-hot-encoding. In one hot-encoding we create a dummy binary variable for each level of a categorical variable. For example we can represent the primary color variable by three binary dummy variables, one for each color (red, blue, and green) . If the color is red, then the variable red takes value 1 while blue and green both take the value zero.Do one-hot-encoding of all your unordered categorical variables (except the target variable y). You can use the function one_hot from mltools package to one-hot encode all categorical variables in a dataset.

Please refer to https://rdrr.io/cran/mltools/man/one_hot.html . Use option DropUnusedLevels=True to avoid creating a binary variable for unused levels of a factor variable.
Please note that the one_hot function takes a data table not a dataframe. You can convert a dataframe to datatable by using as.data.table method https://www.rdocumentation.org/packages/data.table/versions/1.12.8/topics/as.data.table. Make sure to use library(data.table) before using as.data.table method. You can covert a datatable back to a dataframe by
using as.data.frame method https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/as.data.frame

```{R}
bank_dataset[sapply(bank_dataset, is.character)] = lapply(bank_dataset[sapply(bank_dataset, is.character)], as.factor)

str(bank_dataset)
```
```{R}




library(data.table)
library(mltools)

# # Convert dataframe to datatable
# bank_dt <- as.data.table(bank_dataset)
# 
# # One-hot encode unordered categorical variables
# bank_dt_encoded <- one_hot(bank_dt, dropUnusedLevels = TRUE)
# 
# # Convert back to dataframe
# bank_dataset_encoded <- as.data.frame(bank_dt_encoded)

bank_dataset_encoded_x = bank_dataset[, -15]
bank_dataset_encoded_y = bank_dataset['y']


bank_dataset_encoded_x = one_hot(as.data.table(bank_dataset_encoded_x),dropUnusedLevels = TRUE)
bank_dataset_encoded_x= as.data.frame(bank_dataset_encoded_x)


```


```{R}


library(data.table)
library(mltools)
```


Training and Evaluation of ML models

10. Split the data into training and test sets. Use the first 36168 rows for training and the rest for testing.

```{R}

library(caret)

set.seed(123) # set random seed for reproducibility

# create indices for train and test sets
train_idx <- 1:36168
test_idx <- (36168 + 1):nrow(bank_dataset_encoded_x)

dim(bank_dataset_encoded_y)


```


```{R}


# split the data into train and test sets
train_data <- bank_dataset_encoded_x[train_idx, ]
test_data <- bank_dataset_encoded_x[test_idx, ]

train_data_y <- bank_dataset_encoded_y[train_idx, ]
test_data_y <- bank_dataset_encoded_y[test_idx, ]

# Select only numeric columns in train_data
num_cols <- sapply(train_data, is.numeric)
bank_train_num <- train_data[, num_cols]


```


11.(2 pt) Scale all numeric features using z-score normalization. Note: Don’t normalize your one-hotencoded variables. 



```{R}

# Scale the numeric features in train_data using z-score normalization
bank_train_num_scaled <- as.data.frame(scale(bank_train_num))

# Combine the scaled numeric features with the one-hot-encoded variables in train_data
bank_train_scaled <- cbind(bank_train_num_scaled, train_data[!num_cols])

# Select only numeric columns in test_data
num_cols_test <- sapply(test_data, is.numeric) 
bank_test_num <- test_data[, num_cols_test]

# Scale the numeric features in test_data using z-score normalization
bank_test_num_scaled <- as.data.frame(scale(bank_test_num))

# Combine the scaled numeric features with the one-hot-encoded variables in test_data
bank_test_scaled <- cbind(bank_test_num_scaled, test_data[!num_cols_test])


```



12.(3 pts) Use 5-fold cross validation with KNN on the training set to predict the “y” variable and report the cross-validation accuracy. ( Please use crossValidationError function in slides 51-53 of module 4 lecture notes and modify it to compute accuracy instead of error. The accuracy is simply 1- error).

```{R}

#install.packages("caret")
library(class)
library(caret)

```


```{R}

# modify knn_fold function to return accuracy instead of error
knn_fold_acc <- function(features, target, fold, k){
  train <- features[-fold, ]
  validation <- features[fold, ]
  train_labels <- target[-fold]
  validation_labels <- target[fold]
  validation_preds <- knn(train, validation, train_labels, k = k)
  t <- table(validation_labels, validation_preds)
  accuracy <- (t[1, 1] + t[2, 2]) / sum(t)
  return(accuracy)
}

# define cross-validation function to compute average accuracy over all folds
crossValidationAccuracy <- function(features, target, k){
  folds <- createFolds(target, k = 5)
  accuracies <- sapply(folds, knn_fold_acc, features = features, target = target, k = k)
  return(mean(accuracies))
}

# apply cross-validation to the training set
set.seed(123) # set seed for reproducibility
set.seed(123) # set seed for reproducibility
accuracy <- crossValidationAccuracy(bank_train_scaled, train_data_y , k = 5)
accuracy


```



13.(2 pts) Tune K (the number of nearest neighbors) by trying out different values (starting from k=1
to k=sqrt(n) where n is the number of observations in the training set (for example k=1,5,10,20
50,100, sqrt(n) ). Draw a plot of cross validation accuracy for different values of K. Which value of
K seems to perform the best on this data set? (Note: the higher the cross validation accuracy ( or the
lower the cross validation error) the better is the model. You can find an example in slides 54-55
of module 4 lecture notes) Note: This might take several minutes to run on your machine, be
patient.


```{R}

# # create a sequence of k values to try
# ks <- c(1, 5, 10, 20,40)
# 
# # apply cross-validation with each k value and record accuracy
# accuracy_list <- list()
# for (k in k_seq) {
#   set.seed(123)
#   accuracy <- crossValidationAccuracy(bank_train_scaled, bank_dataset_encoded_y, k)
# 
# # plot the accuracy vs. k
# plot(x = k_seq, y = unlist(accuracy_list), xlab = "K", ylab = "Accuracy")

# define function to compute cross-validation accuracy for a given value of k
knn_cv_acc <- function(k, train_features, train_target){
  set.seed(123) # set seed for reproducibility
  return(crossValidationAccuracy(train_features, train_target, k))
}

# compute range of k values to try
n <- nrow(bank_train_scaled)

k_range <- c(1, 5, 10, 20, 40, 60, 80)

# compute cross-validation accuracy for each value of k
cv_accuracies <- sapply(k_range, knn_cv_acc, train_features = bank_train_scaled, train_target = train_data_y)

# plot cross-validation accuracy vs. k
plot(cv_accuracies ~ k_range, main = "Cross-validation accuracy vs. k", xlab = "k", ylab = "Accuracy")
lines(cv_accuracies ~ k_range)



```

14.(3 pt) Use “knn” function to train a knn model on the training set using the best value of K you
found above and get the predicted values for the target variable y in the test set.


```{R}
# Train KNN model using the best value of K
best_k <- 40
knn_model <- knn(bank_train_scaled, bank_test_scaled, train_data_y, k = best_k)

# Get predicted values for the target variable y in the test set
test_data_y_pred <- as.factor(knn_model)

```

```{R}

predicted_y <- knn(bank_train_scaled, bank_test_scaled, train_data_y, k = best_k)

```

15.(2pt) Compare the predicted target (y) with the true target (y) in the test set using a cross
table.

```{R}


table(test_data_y, test_data_y_pred)


```

**The cross table shows the predicted target values (test_data_y_pred) against the true target values (test_data_y) in the test set. There are 7887 true negatives (predicted as "no" and actually "no"), 114 false positives (predicted as "yes" but actually "no"), 834 false negatives (predicted as "no" but actually "yes"), and 208 true positives (predicted as "yes" and actually "yes")**


16.(2 pt) Based on the cross table above, what is the False Positive Rate and False negative Rate
of the knn classifier on the test data? False Positive Rate (FPR) is the percentage of all true
negative (y=”no”) observations that the model predicted to be positive (y=”yes”). False
Negative Rate (FNR) is the percentage of all true positive (y=”yes”) observations that the
model predicted to be negative (y=”no”). FPR and FNR should be values in the range [0-
1].

```{R}

# The False Positive Rate (FPR) is the ratio of false positives (i.e., the number of observations that were actually negative but were predicted as positive) to all actual negatives. Mathematically, it can be computed as follows:
# 
# FPR = FP / N,
# 
# where FP is the number of false positives and N is the total number of actual negatives.
# 
# From the cross table, we can see that there were 114 false positives and 7887 true negatives. Therefore, the FPR can be computed as:
# 
# FPR = 114 / (114 + 7887) = 0.014
# 
# So the False Positive Rate of the KNN classifier on the test data is 0.014 or 1.4%.
# 
# The False Negative Rate (FNR) is the ratio of false negatives (i.e., the number of observations that were actually positive but were predicted as negative) to all actual positives. Mathematically, it can be computed as follows:
# 
# FNR = FN / P,
# 
# where FN is the number of false negatives and P is the total number of actual positives.
# 
# From the cross table, we can see that there were 834 false negatives and 208 true positives. Therefore, the FNR can be computed as:
# 
# FNR = 834 / (834 + 208) = 0.801
# 
# So the False Negative Rate of the KNN classifier on the test data is 0.801 or 80.1%.


```




17.(2 pt) Consider a majority classifier which predicts y=”no” for all observations in the test set.Without
writing any code, explain whatwould be the accuracy of this majority classifier? Does KNN do better
than this majority classifier?

```{R}
# 
# If the accuracy of the KNN model is higher than the accuracy of the majority classifier, then KNN does better than the majority classifier. If the accuracy of the KNN model is lower than the accuracy of the majority classifier, then KNN does worse than the majority classifier. If the accuracy of the KNN model is equal to the accuracy of the majority classifier, then KNN does as well as the majority classifier.

```


18.(2 pt) Explain what isthe False Positive Rate and False Negative Rate of the majority classifier on the test
set and how doesit compare to the FPR and FNR of the knn model you computed in question 16

```{R}

# For the majority classifier which predicts y="no" for all observations in the test set, the False Positive Rate (FPR) would be 0 since it never predicts a positive class (y="yes"). The False Negative Rate (FNR) would be 1 since it predicts all positive cases (y="yes") as negative (y="no").
# 
# When we compare the FPR and FNR of the majority classifier to the FPR and FNR of the KNN model we computed in question 16, we can see that the KNN model has a much lower FPR and FNR. This means that the KNN model is better at correctly predicting both positive and negative cases than the majority classifier.

```





**Problem 2: Applying Naïve Bayes classifier to sentiment classification of COVID tweets**

1. (1pt) Read the data and store in in the dataframe. Take a look at the structure of data and its variables. We
will be working with only two variables: OriginalTweet and Sentiment. Original tweet is a text and Sentiment is a categorical variable with five levels: “extremely positive”, “positive”, “neutral”,“negative”, and “extremely negative”.Note: The original tweet variable hassome accented characterstrings. Set fileEncoding="latin1" parameter inside the read.csv method to ensure those characters are read correctly.


```{R}

corona_dataset = read.csv("C:/ML/Corona_NLP_train.csv",fileEncoding="latin1", stringsAsFactors = FALSE, header=TRUE)

str(corona_dataset)

```

2. Randomize the order of the rows

```{R}

corona_dataset = corona_dataset[sample(nrow(corona_dataset)),]

```

3. (1pt) Convert sentiment into a factor variable with three levels: “positive, “neutral”, and “negative”. You
can do this by labeling all “positive” and “extremely positive” tweets as “positive” and all “negative” and
“extremely negative” tweets as “negative”. Now take the “summary” of sentiment to see how many
observations/tweets you have for each label.

```{R}

# # Convert sentiment into a factor variable with three levels
# corona_dataset %>%
#   mutate(Sentiment = case_when(
#     Sentiment == "Extremely Positive" ~ "Positive",
#     Sentiment == "Extremely Negative" ~ "Negative",
#     TRUE ~ Sentiment
#   )) %>%
#   mutate(Sentiment = factor(Sentiment, levels = c("Positive", "Neutral", "Negative"))) %>%
#   count(Sentiment)

# Convert sentiment into a factor variable with three levels
corona_dataset$Sentiment <- factor(
  ifelse(corona_dataset$Sentiment == "Extremely Positive", "Positive",
         ifelse(corona_dataset$Sentiment == "Extremely Negative", "Negative",
                corona_dataset$Sentiment)),
  levels = c("Positive", "Neutral", "Negative")
)

# Count the number of observations in each sentiment level
table(corona_dataset$Sentiment)




```

4.(2pt) Create a text corpus from OriginalTweet variable. Then clean the corpus, that is convert all tweets to
lowercase, stem and remove stop words, punctuations, and additional white spaces

```{R}

# install.packages("tidyverse")
# install.packages("tm")
# install.packages("SnowballC")
# install.packages("wordcloud")

library(tm)
library(SnowballC)
library(stringr)

# Create a text corpus from OriginalTweet variable
corpus <- VCorpus(VectorSource(corona_dataset$OriginalTweet))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
cleaned_corpus <- unlist(lapply(corpus, as.character))



#corona_dataset$cleaned_text <- cleaned_corpus
#summary(factor(corona_dataset$Sentiment))


```

5. (2pt)Create separate wordclouds for “positive” and “negative” tweets (set max.words=100 to only show
the 100 most frequent words) Is there any visible difference between the frequent words in “positive” vs
“negative” tweets?

```{R}

library(tm)
library(wordcloud)

# Add cleaned text to dataset
corona_dataset$clean_text <- cleaned_corpus

# Create separate wordclouds for "positive" and "negative" tweets
positive_data <- subset(corona_dataset, Sentiment == "Positive")
negative_data <- subset(corona_dataset, Sentiment == "Negative")
positive_corpus <- Corpus(VectorSource(positive_data$clean_text))
negative_corpus <- Corpus(VectorSource(negative_data$clean_text))
positive_corpus <- tm_map(positive_corpus, content_transformer(tolower))
positive_corpus <- tm_map(positive_corpus, removePunctuation)
positive_corpus <- tm_map(positive_corpus, removeNumbers)
positive_corpus <- tm_map(positive_corpus, removeWords, stopwords("english"))
positive_corpus <- tm_map(positive_corpus, stemDocument)
negative_corpus <- tm_map(negative_corpus, content_transformer(tolower))
negative_corpus <- tm_map(negative_corpus, removePunctuation)
negative_corpus <- tm_map(negative_corpus, removeNumbers)
negative_corpus <- tm_map(negative_corpus, removeWords, stopwords("english"))
negative_corpus <- tm_map(negative_corpus, stemDocument)
dtm_positive <- DocumentTermMatrix(positive_corpus, control = list(minWordLength = 1, minDocFreq = 5))
dtm_negative <- DocumentTermMatrix(negative_corpus, control = list(minWordLength = 1, minDocFreq = 5))
m_positive <- as.matrix(dtm_positive)
m_negative <- as.matrix(dtm_negative)
freq_positive <- sort(colSums(m_positive), decreasing = TRUE)
freq_negative <- sort(colSums(m_negative), decreasing = TRUE)
wordcloud(words = names(freq_positive), freq = freq_positive, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"), scale=c(3, 0.5))
wordcloud(words = names(freq_negative), freq = freq_negative, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"), scale=c(3, 0.5))



# Create wordclouds for the top 100 most frequent words in positive and negative tweets
#wordcloud(words = names(freq_positive), freq = freq_positive, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"), scale=c(3, 0.5))
#wordcloud(words = names(freq_negative), freq = freq_negative, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"), scale=c(3, 0.5))




```

6. (1pt) Create a document-term matrix from the cleaned corpus. Then split the data into train and test sets.
Use 80% of samples (roughly 32925 rows ) for training and the rest for testing.



```{R}

library(caret)

set.seed(123) # for reproducibility

#corona_document <- DocumentTermMatrix(cleaned_corpus)
corona_document = DocumentTermMatrix (cleaned_corpus)
#corona_document <- DocumentTermMatrix(cleaned_corpus, weighting = weightTf)


# # Split data into train and test sets using caret
# train_index <- createDataPartition(corona_dataset$Sentiment, p = 0.8, list = FALSE)
# corona_train <- corona_dataset[train_index, ]
# corona_train_labels <- corona_dataset$Sentiment[train_index]
# corona_test <- corona_dataset[-train_index, ]
# corona_test_labels <- corona_dataset$Sentiment[-train_index]

corona_dtm_train <- corona_document[1:32925, ]
corona_dtm_test <- corona_document[32926:41157, ]

corona_train_labels <- corona_dataset[1:32925, ]$Sentiment
corona_test_labels <- corona_dataset[32926:41157, ]$Sentiment

```



7. Remove the words that appear less than 100 times in the training data. Convert frequencies in the document-term matrix to binary yes/no features.

```{R}


library(tm)

# Find terms that appear at least 100 times in the training data
freq_words <- findFreqTerms(corona_dtm_train, 100)

# Remove infrequent terms from the document-term matrix
corona_dtm_train <- corona_dtm_train[, freq_words]
corona_dtm_test <- corona_dtm_test[, freq_words]

# Convert frequencies to binary yes/no features
corona_dtm_train <- weightBin(corona_dtm_train)
corona_dtm_test <- weightBin(corona_dtm_test)






```



8. Train a Naïve Bayes classifier on the training data and evaluate its performance on the test data. Ues a
cross table between the model’s predictions on the test data and the true test labels. Be patient, training and testing will take a while to run. Answer the following questions:

• (1pt) What is the overall accuracy of the model? ( the percentage of correct predictions)
• (3 pt) What is the precision and recall of the model in each category(negative, positive, neutral) ?
precision and Recall are two popular metrics for measuring the performance of a classifier on each class and
they are computed as follows:

```{R}
# library(e1071)
# corona_nb <- naiveBayes(corona_dtm_train, corona_train_labels)
# corona_pred <- predict(corona_nb, corona_test)

# Convert DocumentTermMatrix to data frame
corona_dtm_train_df <- as.data.frame(as.matrix(corona_dtm_train))
corona_dtm_test_df <- as.data.frame(as.matrix(corona_dtm_test))

# Train Naive Bayes classifier
library(e1071)
corona_nb <- naiveBayes(corona_dtm_train_df, corona_train_labels)

# Make predictions on test data
corona_pred <- predict(corona_nb, corona_dtm_test_df)

# Calculate accuracy
corona_accuracy <- mean(corona_pred == corona_test_labels)
cat("Overall accuracy:", corona_accuracy, "\n")

# Create confusion matrix
corona_conf_mat <- table(corona_test_labels, corona_pred)
corona_conf_mat



```

```{R}

# Convert variables to factors with same levels
corona_pred_factor <- factor(corona_pred, levels = levels(factor(corona_test_labels)))
corona_test_labels_factor <- factor(corona_test_labels, levels = levels(factor(corona_pred)))

# Calculate precision and recall for each category
library(caret)
corona_metrics <- confusionMatrix(corona_pred_factor, corona_test_labels_factor, mode = "prec_recall")
corona_metrics$byClass[, c("Precision", "Recall")]



```




